{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d77e79f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/jupyterlab/3.3.2/libexec/lib/python3.9/site-packages/seaborn/rcmod.py:82: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(mpl.__version__) >= \"3.0\":\n",
      "/opt/homebrew/Cellar/jupyterlab/3.3.2/libexec/lib/python3.9/site-packages/setuptools/_distutils/version.py:351: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "import spacy\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# libraries for visualization\n",
    "import pyLDAvis\n",
    "import PyPDF2\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad89e03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "#from sklearn.externals import joblib\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37073bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"CLEANING\"\"\"\n",
    "\n",
    "def clean_text(t): \n",
    "    delete_dict = {sp_character: '' for sp_character in string.punctuation}\n",
    "    delete_dict[' '] = ' ' \n",
    "    txt = ' '.join(map(str,t))\n",
    "    table = str.maketrans(delete_dict)\n",
    "    text1 = txt.translate(table)\n",
    "    textArr= text1.split()\n",
    "    text2 = ' '.join([w for w in textArr if ( not w.isdigit() and  ( not w.isdigit() and len(w)>3))]) \n",
    "    return list(text2.lower().split('|'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cd7a33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"REMOVING STOPWORDS\"\"\"\n",
    "\n",
    "def remove_stopwords(t):\n",
    "    en = spacy.load('en_core_web_sm')\n",
    "    stpwrds = en.Defaults.stop_words\n",
    "    all_stopwords = stpwrds.union(stopwords.words('english'))\n",
    "    finalList = []\n",
    "    for i in t:\n",
    "        textArr = i.split(' ')    \n",
    "        rem_text = ' '.join([i for i in textArr if i not in all_stopwords])\n",
    "        finalList.append(rem_text)\n",
    "    return finalList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dea60d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"LEMMATIZATION\"\"\"\n",
    "\n",
    "def lemmatize(t):\n",
    "    nlp = spacy.load('en_core_web_sm', disable=['parser','ner'])\n",
    "    #text_list = text.tolist()\n",
    "    splem = []\n",
    "    wordlist = []\n",
    "    for i in t:\n",
    "        wordlist.append(i)\n",
    "        lst = word_tokenize(i)\n",
    "        #print(lst)\n",
    "        #wordlist.append(lst)\n",
    "        doc = nlp(' '.join(map(str,lst)))\n",
    "        lem = \" \".join([token.lemma_ for token in doc])\n",
    "        #print(lem)\n",
    "        splem.append(lem)\n",
    "    #return(' '.join(map(str,splem)))\n",
    "    return splem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8d9b62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"SPLITTING THE LEMMATIZED TEXTS INTO TOKENS\"\"\"\n",
    "\n",
    "# def make_tokens(text):\n",
    "#     tokens = [d.split() for d in text]\n",
    "#     return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac9fbc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Making the doc_term_matrix\"\"\"\n",
    "def mtrx(t):\n",
    "    tokens = [d.split() for d in t]\n",
    "    dictionary = corpora.Dictionary(tokens)\n",
    "    doc_term_matrix = [dictionary.doc2bow(rev) for rev in tokens]\n",
    "    return doc_term_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12526ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"MODEL BUILDING\"\"\"\n",
    "\n",
    "def lda_model_build(t,doc_term_matrix):\n",
    "    tokens = [d.split() for d in t]\n",
    "    dictionary = corpora.Dictionary(tokens)\n",
    "    #doc_term_matrix = [dictionary.doc2bow(rev) for rev in tokens]\n",
    "    LDA = gensim.models.ldamodel.LdaModel\n",
    "    lda_model = LDA(corpus=doc_term_matrix, id2word=dictionary, num_topics=10, random_state=100,\n",
    "                chunksize=1000, passes=50,iterations=100)\n",
    "    return lda_model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19ff6a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_topics(lda_model):\n",
    "    print(lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d8a864f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"SHOW THE DOMINANT TOPIC WITH THE KEYWORDS\"\"\"\n",
    "\n",
    "def format_topics_sentences(ldamodel, corpus, texts):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    df_dominant_topic = sent_topics_df.reset_index()\n",
    "    df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "    return df_dominant_topic\n",
    "\n",
    "\n",
    "# df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=doc_term_matrix, texts=for_end)\n",
    "\n",
    "# # Format\n",
    "# df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "# df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "# # Show\n",
    "# df_dominant_topic.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98c93033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.016*\"order\" + 0.016*\"nail\" + 0.016*\"problem\" + 0.016*\"play\" + 0.016*\"phase\" + 0.016*\"people\" + 0.016*\"pandemic\" + 0.016*\"production\" + 0.016*\"monetization\" + 0.016*\"mean\"'), (1, '0.016*\"order\" + 0.016*\"nail\" + 0.016*\"problem\" + 0.016*\"play\" + 0.016*\"phase\" + 0.016*\"people\" + 0.016*\"pandemic\" + 0.016*\"production\" + 0.016*\"monetization\" + 0.016*\"mean\"'), (2, '0.037*\"impact\" + 0.037*\"economic\" + 0.037*\"deep\" + 0.025*\"order\" + 0.025*\"crisis\" + 0.025*\"production\" + 0.025*\"problem\" + 0.025*\"export\" + 0.025*\"mean\" + 0.025*\"economy\"'), (3, '0.016*\"deep\" + 0.016*\"economic\" + 0.016*\"impact\" + 0.016*\"mean\" + 0.016*\"production\" + 0.016*\"economy\" + 0.016*\"export\" + 0.016*\"msme\" + 0.016*\"crisis\" + 0.016*\"problem\"'), (4, '0.016*\"order\" + 0.016*\"nail\" + 0.016*\"problem\" + 0.016*\"play\" + 0.016*\"phase\" + 0.016*\"people\" + 0.016*\"pandemic\" + 0.016*\"production\" + 0.016*\"monetization\" + 0.016*\"mean\"'), (5, '0.016*\"order\" + 0.016*\"nail\" + 0.016*\"problem\" + 0.016*\"play\" + 0.016*\"phase\" + 0.016*\"people\" + 0.016*\"pandemic\" + 0.016*\"production\" + 0.016*\"monetization\" + 0.016*\"mean\"'), (6, '0.016*\"order\" + 0.016*\"nail\" + 0.016*\"problem\" + 0.016*\"play\" + 0.016*\"phase\" + 0.016*\"people\" + 0.016*\"pandemic\" + 0.016*\"production\" + 0.016*\"monetization\" + 0.016*\"mean\"'), (7, '0.016*\"deep\" + 0.016*\"economic\" + 0.016*\"msme\" + 0.016*\"impact\" + 0.016*\"economy\" + 0.016*\"crisis\" + 0.016*\"production\" + 0.016*\"export\" + 0.016*\"mean\" + 0.016*\"problem\"'), (8, '0.016*\"order\" + 0.016*\"nail\" + 0.016*\"problem\" + 0.016*\"play\" + 0.016*\"phase\" + 0.016*\"people\" + 0.016*\"pandemic\" + 0.016*\"production\" + 0.016*\"monetization\" + 0.016*\"mean\"'), (9, '0.016*\"order\" + 0.016*\"nail\" + 0.016*\"problem\" + 0.016*\"play\" + 0.016*\"phase\" + 0.016*\"people\" + 0.016*\"pandemic\" + 0.016*\"production\" + 0.016*\"monetization\" + 0.016*\"mean\"')]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_No</th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.9886</td>\n",
       "      <td>impact, economic, deep, order, crisis, production, problem, export, mean, economy</td>\n",
       "      <td>Covid 19 pandemic  has had deep economic repercussions and has triggered  a deep economic crisis It's important to think about three phases as it were of how this crisis is going to play out in the economic domain the first is what began to happen in about February 2020 when we saw China shutdown subsequently with a lot of people cancelling orders for textile exports from India which was kind of a collapse in world trade that would impact both  production and demand domestically so it means that your exports are lower but it also means that you're not able to produce things that require. The consequences of the of the lockdown in order to control the virus sort of put the economy in the freeze and definitely there is going to be a very large impact on production and incomes going forward and we have to deal with it as a multi-step problem because it's not just an impact on the entire economy takes there's layers to it. MSME’s have been in deep problems ever since the monetization so this was basically the final nail in the coffin and unless we take a few steps/measures quickly a lot MSME are simply going to die.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Document_No  Dominant_Topic  Topic_Perc_Contrib  \\\n",
       "0  0            2               0.9886               \n",
       "\n",
       "                                                                            Keywords  \\\n",
       "0  impact, economic, deep, order, crisis, production, problem, export, mean, economy   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Text  \n",
       "0  Covid 19 pandemic  has had deep economic repercussions and has triggered  a deep economic crisis It's important to think about three phases as it were of how this crisis is going to play out in the economic domain the first is what began to happen in about February 2020 when we saw China shutdown subsequently with a lot of people cancelling orders for textile exports from India which was kind of a collapse in world trade that would impact both  production and demand domestically so it means that your exports are lower but it also means that you're not able to produce things that require. The consequences of the of the lockdown in order to control the virus sort of put the economy in the freeze and definitely there is going to be a very large impact on production and incomes going forward and we have to deal with it as a multi-step problem because it's not just an impact on the entire economy takes there's layers to it. MSME’s have been in deep problems ever since the monetization so this was basically the final nail in the coffin and unless we take a few steps/measures quickly a lot MSME are simply going to die.  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "sentence = \"Covid 19 pandemic  has had deep economic repercussions and has triggered  a deep economic crisis It's important to think about three phases as it were of how this crisis is going to play out in the economic domain the first is what began to happen in about February 2020 when we saw China shutdown subsequently with a lot of people cancelling orders for textile exports from India which was kind of a collapse in world trade that would impact both  production and demand domestically so it means that your exports are lower but it also means that you're not able to produce things that require. The consequences of the of the lockdown in order to control the virus sort of put the economy in the freeze and definitely there is going to be a very large impact on production and incomes going forward and we have to deal with it as a multi-step problem because it's not just an impact on the entire economy takes there's layers to it. MSME’s have been in deep problems ever since the monetization so this was basically the final nail in the coffin and unless we take a few steps/measures quickly a lot MSME are simply going to die.\"\n",
    "ans = chnge_to_list(sentence)\n",
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3088369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = chnge_to_list(text)\n",
    "# b = clean_text(a)\n",
    "# c = remove_stopwords(b)\n",
    "# d = lemmatize(c)\n",
    "# e = make_tokens(d)\n",
    "# f = mtrx(e)\n",
    "# g = lda_model_build(f)\n",
    "# h = format_topics_sentences(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4536ed48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline = Pipeline([('list',chnge_to_list(text)),\n",
    "#                     ('clean',clean_text(chnge_to_list(text))),\n",
    "#                     ('remove stopwords',remove_stopwords(clean_text(chnge_to_list(text)))),\n",
    "#                     ('lematize',lemmatize(text)),\n",
    "#                     ('token',make_tokens(text)),\n",
    "#                     ('mtrx',mtrx(tokens)),\n",
    "#                     ('model',lda_model_build(doc_term_matrix)),\n",
    "#                     ('dominant',format_topics_sentences(ldamodel=lda_model, corpus=doc_term_matrix, texts=for_end))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5129aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chnge_to_list(text):\n",
    "    sample_list = list(text.split('|'))\n",
    "    #for_end = list(text)\n",
    "    a = clean_text(sample_list)\n",
    "    b = remove_stopwords(a)\n",
    "    c = lemmatize(b)\n",
    "    d = remove_stopwords(c)\n",
    "    e = mtrx(d)\n",
    "    f = lda_model_build(d,e)\n",
    "    g = print_topics(f)\n",
    "    g = format_topics_sentences(ldamodel=f, corpus=e, texts=sample_list)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541c477d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
